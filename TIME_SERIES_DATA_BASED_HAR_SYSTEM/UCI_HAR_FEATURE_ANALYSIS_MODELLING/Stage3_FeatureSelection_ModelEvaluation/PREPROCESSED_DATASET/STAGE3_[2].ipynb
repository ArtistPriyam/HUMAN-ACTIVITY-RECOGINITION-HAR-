{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddea3a4",
   "metadata": {},
   "source": [
    "feature selection techniques\n",
    "\n",
    "including ANOVA, Chi-Squared, Forward Selection, Recursive Feature Elimination (RFE), Random Forestbased selection, and Decision TREE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f71d244",
   "metadata": {},
   "source": [
    "## imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aad0b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === IMPORTS ===\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0503aa31",
   "metadata": {},
   "source": [
    " input variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b00c1219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_85783/549718802.py:4: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_train= pd.read_csv(os.path.join(base_path1, \"train\", \"X_train.csv\"), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_85783/549718802.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_train = pd.read_csv(os.path.join(base_path1, \"train\", \"y_train.csv\"), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_85783/549718802.py:6: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  X_test = pd.read_csv(os.path.join(base_path1, \"test\", \"X_test.csv\"), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_85783/549718802.py:7: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y_test = pd.read_csv(os.path.join(base_path1, \"test\", \"y_test.csv\"), delim_whitespace=True, header=None)\n",
      "/var/folders/z6/4z3b7f1d38db3t0frgwjyknh0000gn/T/ipykernel_85783/549718802.py:8: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  features = pd.read_csv(os.path.join(base_path1, \"features.txt\"), delim_whitespace=True, header=None)\n"
     ]
    }
   ],
   "source": [
    "# Define base path\n",
    "base_path1 = \"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/UCI HAR Dataset\"\n",
    "\n",
    "X_train= pd.read_csv(os.path.join(base_path1, \"train\", \"X_train.csv\"), delim_whitespace=True, header=None)\n",
    "y_train = pd.read_csv(os.path.join(base_path1, \"train\", \"y_train.csv\"), delim_whitespace=True, header=None)\n",
    "X_test = pd.read_csv(os.path.join(base_path1, \"test\", \"X_test.csv\"), delim_whitespace=True, header=None)\n",
    "y_test = pd.read_csv(os.path.join(base_path1, \"test\", \"y_test.csv\"), delim_whitespace=True, header=None)\n",
    "features = pd.read_csv(os.path.join(base_path1, \"features.txt\"), delim_whitespace=True, header=None)\n",
    "# attach feature names\n",
    "X_train.columns = features[1].values\n",
    "X_test.columns = features[1].values\n",
    "X= pd.concat([X_train, X_test], axis=0)\n",
    "y = pd.concat([y_train, y_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a1c28a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-meanFreq()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074323</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>0.158075</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 561 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-meanFreq()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.074323   \n",
       "1         -0.957686         -0.943068  ...                         0.158075   \n",
       "\n",
       "   fBodyBodyGyroJerkMag-skewness()  fBodyBodyGyroJerkMag-kurtosis()  \\\n",
       "0                        -0.298676                        -0.710304   \n",
       "1                        -0.595051                        -0.861499   \n",
       "\n",
       "   angle(tBodyAccMean,gravity)  angle(tBodyAccJerkMean),gravityMean)  \\\n",
       "0                    -0.112754                              0.030400   \n",
       "1                     0.053477                             -0.007435   \n",
       "\n",
       "   angle(tBodyGyroMean,gravityMean)  angle(tBodyGyroJerkMean,gravityMean)  \\\n",
       "0                         -0.464761                             -0.018446   \n",
       "1                         -0.732626                              0.703511   \n",
       "\n",
       "   angle(X,gravityMean)  angle(Y,gravityMean)  angle(Z,gravityMean)  \n",
       "0             -0.841247              0.179941             -0.058627  \n",
       "1             -0.844788              0.180289             -0.054317  \n",
       "\n",
       "[2 rows x 561 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15766b4e",
   "metadata": {},
   "source": [
    "# plotting top 20 features for each feature selection method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eef6d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path2= \"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bee9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y = np.array(y_test).ravel()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_test)\n",
    "feature_names = X.columns if isinstance(X, pd.DataFrame) else [f\"f{i}\" for i in range(X.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b962cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_methods = {\n",
    "    \"ANOVA\": SelectKBest(score_func=f_classif, k='all'),\n",
    "    \"ChiSquared\": SelectKBest(score_func=chi2, k='all'),\n",
    "    \"RFE\": RFE(estimator=LogisticRegression(max_iter=500), n_features_to_select=20),\n",
    "    \"ForwardSelection\": SequentialFeatureSelector(estimator=LogisticRegression(max_iter=500), \n",
    "                                                   n_features_to_select=10, direction='forward'),\n",
    "    \"RandomForestImportance\": RandomForestClassifier(),\n",
    "    \"DecisionTreeImportance\": DecisionTreeClassifier()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61a56a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Plot saved to: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD/F-score (ANOVA)_top_20_features_ppd.png\n",
      "âœ” Plot saved to: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD/Chi2 score_top_20_features_ppd.png\n",
      "âœ” Plot saved to: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD/RFE ranking_top_20_features_ppd.png\n",
      "âœ” Plot saved to: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD/Forward selection_top_20_features_ppd.png\n",
      "âœ” Plot saved to: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD/RandomForest Importance_top_20_features_ppd.png\n",
      "âœ” Plot saved to: /Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/TOP_20_FEATURES_PLOT_PPD/DecisionTree Importance_top_20_features_ppd.png\n"
     ]
    }
   ],
   "source": [
    "scores = {}\n",
    "\n",
    "# ANOVA F-score\n",
    "anova_selector = feature_methods[\"ANOVA\"].fit(X_scaled, y)\n",
    "scores[\"F-score (ANOVA)\"] = anova_selector.scores_\n",
    "\n",
    "# Chi2\n",
    "chi2_selector = feature_methods[\"ChiSquared\"].fit(X_scaled, y)\n",
    "scores[\"Chi2 score\"] = chi2_selector.scores_\n",
    "\n",
    "# RFE\n",
    "rfe_selector = feature_methods[\"RFE\"].fit(X_scaled, y)\n",
    "scores[\"RFE ranking\"] = -rfe_selector.ranking_  # lower is better â†’ invert for plotting\n",
    "\n",
    "# Forward Selection\n",
    "fs_selector = feature_methods[\"ForwardSelection\"].fit(X_scaled, y)\n",
    "scores[\"Forward selection\"] = fs_selector.get_support().astype(int)  # 1 if selected\n",
    "\n",
    "# Random Forest Importance\n",
    "rf_model = feature_methods[\"RandomForestImportance\"].fit(X_scaled, y)\n",
    "scores[\"RandomForest Importance\"] = rf_model.feature_importances_\n",
    "\n",
    "# Decision Tree Importance\n",
    "dt_model = feature_methods[\"DecisionTreeImportance\"].fit(X_scaled, y)\n",
    "scores[\"DecisionTree Importance\"] = dt_model.feature_importances_\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame(scores, index=feature_names)\n",
    "\n",
    "for score_type in score_df.columns:\n",
    "    top_20 = score_df[score_type].nlargest(20)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_20.plot(kind='barh')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f\"Top 20 Features by {score_type}\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    \n",
    "    plot_path = os.path.join(base_path2, f\"{score_type}_top_20_features_ppd.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "    print(f\"âœ” Plot saved to: {plot_path}\")\n",
    "    # Save the DataFrame to a CSV file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87298b32",
   "metadata": {},
   "source": [
    "# model training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "583eba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path= \"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/Stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/COMPARISON_TABLES_PPD\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a21e6b",
   "metadata": {},
   "source": [
    " defining the models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef31354",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"LinearSVC\": LinearSVC(max_iter=500),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_jobs=-1),\n",
    "    \"BaggedDecisionTrees\": BaggingClassifier(n_jobs=-1),\n",
    "    \"ANN\": MLPClassifier(max_iter=300),\n",
    "    \"CART\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(n_jobs=-1),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=500),\n",
    "    \"RandomForest\": RandomForestClassifier(n_jobs=-1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f574d7",
   "metadata": {},
   "source": [
    " FETAURE SELCTION METHODS list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce9186",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7385412e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_methods = {\n",
    "    \"ANOVA\": SelectKBest(score_func=f_classif, k=20),\n",
    "    \"ChiSquared\": SelectKBest(score_func=chi2, k=20),\n",
    "    \"RFE\": RFE(estimator=LogisticRegression(max_iter=500), n_features_to_select=20),\n",
    "    \"ForwardSelection\": SequentialFeatureSelector(\n",
    "        estimator=LogisticRegression(max_iter=500), \n",
    "        n_features_to_select=20, \n",
    "        direction='forward'\n",
    "    ),\n",
    "    \"RandomForestImportance\": RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    \"DecisionTreeImportance\": DecisionTreeClassifier(random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c21f199",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ca90fb9",
   "metadata": {},
   "source": [
    " MODEL EVALUATION function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "228cade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EVALUATION FUNCTION ===\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted')\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2e4c58",
   "metadata": {},
   "source": [
    "### MAIN LOOP FOR FEATURE SELECTION ,MODEL TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06450d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27c1fec",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d7d5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop [2]\n",
    "base_path= \"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/Stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/COMPARISON_TABLES_PPD\"\n",
    "models = {\n",
    "    \"LinearSVC\": LinearSVC(max_iter=500),\n",
    "    \"GradientBoosting\": GradientBoostingClassifier(),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(n_jobs=-1),\n",
    "    \"BaggedDecisionTrees\": BaggingClassifier(n_jobs=-1),\n",
    "    \"ANN\": MLPClassifier(max_iter=300),\n",
    "    \"CART\": DecisionTreeClassifier(),\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"AdaBoost\": AdaBoostClassifier(),\n",
    "    \"KNN\": KNeighborsClassifier(n_jobs=-1),\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=500),\n",
    "    \"RandomForest\": RandomForestClassifier(n_jobs=-1),\n",
    "}\n",
    "\n",
    "feature_methods = {\n",
    "    #\"ANOVA\": SelectKBest(score_func=f_classif, k=20),\n",
    "    #\"ChiSquared\": SelectKBest(score_func=chi2, k=20),\n",
    "    #\"RFE\": RFE(estimator=LogisticRegression(max_iter=500), n_features_to_select=20),\n",
    "    '''\n",
    "    \"ForwardSelection\": SequentialFeatureSelector(\n",
    "        estimator=LogisticRegression(max_iter=500), \n",
    "        n_features_to_select=20, \n",
    "        direction='forward'\n",
    "    ), \n",
    "    '''\n",
    "    \"RandomForestImportance\": RandomForestClassifier(n_jobs=-1, random_state=42),\n",
    "    \"DecisionTreeImportance\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "# === EVALUATION FUNCTION ===\n",
    "def evaluate_model(y_test, y_pred):\n",
    "    return {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred, average='weighted'),\n",
    "        \"Precision\": precision_score(y_test, y_pred, average='weighted'),\n",
    "        \"Recall\": recall_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "\n",
    "feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else [f\"Feature {i}\" for i in range(X.shape[1])]\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "for method_name, selector in feature_methods.items():\n",
    "    print(f\"\\nðŸ” Feature Selection: {method_name}\")\n",
    "\n",
    "    # Normalize for Chi-Squared\n",
    "    if method_name == \"ChiSquared\":\n",
    "        X_scaled = MinMaxScaler().fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = X.copy()\n",
    "\n",
    "    # Feature selection\n",
    "    if method_name in [\"RandomForestImportance\", \"DecisionTreeImportance\"]:\n",
    "        selector.fit(X_scaled, y)\n",
    "        importances = selector.feature_importances_\n",
    "        top_k_indices = np.argsort(importances)[-22:]\n",
    "        selected_features = top_k_indices.tolist()\n",
    "        # Convert to NumPy if DataFrame\n",
    "        if isinstance(X_scaled, pd.DataFrame):\n",
    "            X_selected = X_scaled.iloc[:, top_k_indices].values\n",
    "        else:\n",
    "            X_selected = X_scaled[:, top_k_indices]\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.barh(range(22), importances[top_k_indices])\n",
    "        plt.yticks(ticks=range(22), labels=np.array(feature_names)[top_k_indices])\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(f\"{method_name} - Top 22 Features\")\n",
    "        plot_path = os.path.join(base_path, f\"{method_name}_RESULT\", f\"{method_name}_feature_importance.png\")\n",
    "        os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot_path)\n",
    "        plt.close()\n",
    "    \n",
    "    else:\n",
    "        selector.fit(X_scaled, y)\n",
    "        mask = selector.get_support()\n",
    "        selected_features = np.where(mask)[0].tolist()\n",
    "        # Convert to NumPy if DataFrame\n",
    "        if isinstance(X_scaled, pd.DataFrame):\n",
    "            X_selected = X_scaled.iloc[:, mask].values\n",
    "        else:\n",
    "            X_selected = X_scaled[:, mask]\n",
    "            \n",
    "        \n",
    "    # Save selected features\n",
    "    selected_feature_names = [feature_names[i] for i in selected_features]\n",
    "    output_folder = os.path.join(base_path, f\"{method_name}_RESULT\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    with open(os.path.join(output_folder, f\"{method_name}_selected_features.txt\"), 'w') as f:\n",
    "        for name in selected_feature_names:\n",
    "            f.write(name + '\\n')\n",
    "\n",
    "    # train and evaluate models with selected features\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    results = []\n",
    "    for model_name, model in models.items():\n",
    "        try:\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            scores = evaluate_model(y_test, y_pred)\n",
    "            results.append([model_name, scores[\"Accuracy\"], scores[\"F1\"], scores[\"Precision\"], scores[\"Recall\"]])\n",
    "        except Exception as e:\n",
    "            print(f\" Error in {model_name}: {e}\")\n",
    "            results.append([model_name, \"ERR\", \"ERR\", \"ERR\", \"ERR\"])\n",
    "    # Save results\n",
    "    output_folder = os.path.join(base_path, f\"{method_name}_RESULT\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    df_results = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"F1 Score\", \"Precision\", \"Recall\"])\n",
    "    output_txt_path = os.path.join(output_folder, f\"{method_name}_model_score.txt\")\n",
    "    \n",
    "    table_string = tabulate(df_results, headers='keys', tablefmt='grid', showindex=False)\n",
    "    \n",
    "    with open(output_txt_path, 'w') as f:\n",
    "        f.write(table_string)\n",
    "    \n",
    "    print(f\"âœ” Results saved to: {output_txt_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c120bac",
   "metadata": {},
   "source": [
    "# deciding k value for each feature selection method \n",
    "- using kneed library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4b4a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path4 = \"/Users/priyam/paper_recreation/UCI_HAR_FEATURE_ANALYSIS_MODELLING/stage3_FeatureSelection_ModelEvaluation/PREPROCESSED_DATASET/OUTPUT_PPD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f13258",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_methods = {\n",
    "    \"ANOVA\": SelectKBest(score_func=f_classif, k='all'),\n",
    "    \"ChiSquared\": SelectKBest(score_func=chi2, k='all'),\n",
    "    \"RFE\": RFE(estimator=LogisticRegression(max_iter=500), n_features_to_select=10),\n",
    "    \"ForwardSelection\": SequentialFeatureSelector(estimator=LogisticRegression(max_iter=500),                                          n_features_to_select=10, direction='forward'),\n",
    "    \"RandomForestImportance\": RandomForestClassifier(),\n",
    "    \"DecisionTreeImportance\": DecisionTreeClassifier()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f9b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, chi2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for method_name, selector in feature_methods.items():\n",
    "    print(f\"ðŸ“Š Feature Score Visualization: {method_name}\")\n",
    "    \n",
    "    if method_name == \"ChiSquared\":\n",
    "        X_scaled = MinMaxScaler().fit_transform(X_test)\n",
    "    else:\n",
    "        X_scaled = X.copy()\n",
    "        \n",
    "    if method_name in [\"RandomForestImportance\", \"DecisionTreeImportance\"]:\n",
    "        selector.fit(X_scaled, y)\n",
    "        scores = selector.feature_importances_\n",
    "\n",
    "    elif method_name == \"ANOVA\":\n",
    "        scores, _ = f_classif(X_scaled, y_test)\n",
    "\n",
    "    elif method_name == \"ChiSquared\":\n",
    "        scores, _ = chi2(X_scaled, y_test)\n",
    "\n",
    "    elif method_name == \"RFE\":\n",
    "        selector.fit(X_scaled, y_test)\n",
    "        scores = -selector.ranking_  # lower is better, so invert for plotting\n",
    "\n",
    "    elif method_name == \"ForwardSelection\":\n",
    "        selector.fit(X_scaled, y_test)\n",
    "        scores = selector.get_support().astype(int)  # 1 for selected, 0 otherwise\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "    # index as x axi  Plotting\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(range(X.shape[1]), scores)\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Score / Importance\")\n",
    "    plt.title(f\"{method_name}: Feature Ranking Scores\")\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "\n",
    "    # Save plot\n",
    "    output_folder = os.path.join(base_path4, f\"{method_name}_RESULT\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # Save the plot\n",
    "    plot_path = os.path.join(output_folder, f\"{method_name}_features_scores.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\" Plot saved to: {plot_path}\")\n",
    "    plt.close()  # Close the plot to free memory\n",
    "    # Save feature names and scores\n",
    "\n",
    "\n",
    "'''\n",
    "    # feature nmaes as x axis \n",
    "    if isinstance(X, pd.DataFrame):\n",
    "        feature_names = np.array(X.columns)\n",
    "    else:\n",
    "        feature_names = np.array([f\"Feature {i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    sorted_indices = np.argsort(scores)[::-1]\n",
    "    sorted_scores = scores[sorted_indices]\n",
    "    sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.bar(sorted_feature_names, sorted_scores)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.xlabel(\"Feature Name\")\n",
    "    plt.ylabel(\"Score / Importance\")\n",
    "    plt.title(f\"{method_name} Feature Scores\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b738d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kneed\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf15681",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "\n",
    "suggested_k = {}\n",
    "\n",
    "for method_name, selector in feature_methods.items():\n",
    "    print(f\"ðŸ“Š Feature Score Analysis: {method_name}\")\n",
    "    \n",
    "    if method_name == \"ChiSquared\":\n",
    "        X_scaled = MinMaxScaler().fit_transform(X)\n",
    "    else:\n",
    "        X_scaled = X.copy()\n",
    "\n",
    "    if method_name in [\"RandomForestImportance\", \"DecisionTreeImportance\"]:\n",
    "        selector.fit(X_scaled, y)\n",
    "        scores = selector.feature_importances_\n",
    "\n",
    "    elif method_name == \"ANOVA\":\n",
    "        scores, _ = f_classif(X_scaled, y)\n",
    "\n",
    "    elif method_name == \"ChiSquared\":\n",
    "        scores, _ = chi2(X_scaled, y)\n",
    "\n",
    "    elif method_name == \"RFE\":\n",
    "        selector.fit(X_scaled, y)\n",
    "        scores = -selector.ranking_  # Invert so higher is better\n",
    "\n",
    "    elif method_name == \"ForwardSelection\":\n",
    "        selector.fit(X_scaled, y)\n",
    "        scores = selector.get_support().astype(int)  # Not useful for ranking\n",
    "        suggested_k[method_name] = np.sum(scores)\n",
    "        continue  # skip plotting for binary mask\n",
    "\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    sorted_scores = sorted(scores, reverse=True)\n",
    "    kl = KneeLocator(range(1, len(sorted_scores)+1), sorted_scores, curve=\"convex\", direction=\"decreasing\")\n",
    "    k = kl.knee if kl.knee is not None else 10  # fallback if knee not found\n",
    "    suggested_k[method_name] = k\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(range(1, len(sorted_scores)+1), sorted_scores, marker='o')\n",
    "    if kl.knee:\n",
    "        plt.axvline(k, color='red', linestyle='--', label=f'k = {k}')\n",
    "    plt.title(f\"{method_name} - Score Curve with Elbow\")\n",
    "    plt.xlabel(\"Feature Rank\")\n",
    "    plt.ylabel(\"Score / Importance\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    # Save plot\n",
    "    output_folder = os.path.join(base_path4, f\"{method_name}_RESULT\")\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    plot_path = os.path.join(output_folder, f\"{method_name}_elbow__plot.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    print(f\" Elbow plot saved to: {plot_path}\")\n",
    "    plt.close()  # Close the plot to free memory\n",
    "print(\"\\nðŸ“Œ Suggested `k` values:\")\n",
    "for method, k in suggested_k.items():\n",
    "    print(f\"  {method}: k = {k}\")\n",
    "# Save suggested k values\n",
    "suggested_k_path = os.path.join(base_path4, \"suggested_k_value.csv\")\n",
    "suggested_k_df = pd.DataFrame(list(suggested_k.items()), columns=[\"Method\", \"Suggested k\"])\n",
    "suggested_k_df.to_csv(suggested_k_path, index=False)\n",
    "print(f\"Suggested k values saved to: {suggested_k_path}\")\n",
    "# Save feature names and scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0eb44",
   "metadata": {},
   "source": [
    "#### KNEED AND FEATURE SCORE MEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44612cd4",
   "metadata": {},
   "source": [
    "\n",
    " ðŸ§  **What is `kneed` doing?**\n",
    "\n",
    "**`kneed`** uses the **elbow method** to find the point where a curve (like feature importance or score) **starts to flatten** â€” this point is called the **\"knee\"** or **\"elbow\"**.\n",
    "\n",
    " ðŸ” Why?\n",
    "\n",
    "In feature selection, plotting scores sorted in descending order often shows:\n",
    "\n",
    "* Sharp drops for important features\n",
    "* A plateau for less informative features\n",
    "\n",
    "The **elbow point** is a natural choice for `k`, the number of important features to keep.\n",
    "\n",
    "---\n",
    "\n",
    " ðŸ“‰ How it works internally:\n",
    "\n",
    "* It looks at the curve formed by feature scores.\n",
    "* Fits a **convex/concave curve**.\n",
    "* Finds the point where the **slope changes sharply** â€” this is the \"knee\".\n",
    "\n",
    "So `kneed` helps find this point **programmatically**, instead of visually inspecting every time.\n",
    "\n",
    "---\n",
    "\n",
    " ðŸ“ˆ **What is this \"score\"?** (No heading shown in your plots)\n",
    "\n",
    "It depends on the method:\n",
    "\n",
    "| Feature Method       | \"Score\" Meaning                                             |\n",
    "| -------------------- | ----------------------------------------------------------- |\n",
    "| **ANOVA**            | F-statistic (higher = feature better separates classes)     |\n",
    "| **ChiSquared**       | Chi-squared test statistic (measures dependency with label) |\n",
    "| **RandomForest/DT**  | Gini-based or entropy-based importance                      |\n",
    "| **RFE**              | Feature rank (lower = better), inverted here for plotting   |\n",
    "| **ForwardSelection** | Binary mask (1 = selected, but no ranking available)        |\n",
    "\n",
    "So this **score axis** on your plot is:\n",
    "\n",
    "> How much a feature contributes to predicting the target â€” as defined by each selection method.\n",
    "\n",
    "--- ðŸŽ¯ Summary:\n",
    "\n",
    "* `kneed` uses **curve shape** to find where **score importance levels off**.\n",
    "* The **\"score\"** depends on statistical strength or model-based importance.\n",
    "* This helps pick a smart `k` without arbitrary guessing.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50424d41",
   "metadata": {},
   "source": [
    "The \"score\" represents how important or useful each feature is for predicting the target variable â€” but the exact meaning depends on the method used:\n",
    "\n",
    "* **In ANOVA (f\\_classif)**:\n",
    "  The score is the **F-statistic**, which compares the variance **between groups (classes)** to the variance **within groups**. A high F-value means the feature separates classes well â€” it's strongly correlated with the output.\n",
    "\n",
    "* **In Chi-Squared test (chi2)**:\n",
    "  The score is the **Chi-squared statistic**, which tests **independence** between the feature and the target. A high score means the feature and the target are likely **dependent** (i.e., the feature gives useful info about the target).\n",
    "\n",
    "* **In Random Forest / Decision Tree**:\n",
    "  The score is based on **feature importance**, calculated from how much a feature **reduces impurity (like Gini or entropy)** across all trees. Features that help split the data cleanly (low impurity) get higher importance scores.\n",
    "\n",
    "* **In RFE (Recursive Feature Elimination)**:\n",
    "  This method doesn't give scores directly but ranks features by how important they are to a model (e.g., logistic regression). We convert ranks into scores by taking the **negative rank**, so better-ranked features have higher scores.\n",
    "\n",
    "* **In Forward Selection**:\n",
    "  Thereâ€™s no direct score; it selects a subset of features step-by-step based on which one improves the model most at each step. The result is a binary yes/no for each feature (selected or not), not a numeric score.\n",
    "\n",
    "In all cases, higher scores mean the feature is more useful or informative for the prediction task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8072fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
